{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "164762a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root found at: /shared/home/mauro.diaz/work/FOGAS\n",
      "Loading dataset from: /shared/home/mauro.diaz/work/FOGAS/datasets/test_fqi.csv\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# This starts from the current working directory\n",
    "# and goes up until it finds the 'fogas_torch' folder or '.git'\n",
    "def find_root(current_path, marker=\"fogas_torch\"):\n",
    "    current_path = Path(current_path).resolve()\n",
    "    for parent in [current_path] + list(current_path.parents):\n",
    "        if (parent / marker).exists():\n",
    "            return parent\n",
    "    return current_path\n",
    "\n",
    "PROJECT_ROOT = find_root(Path.cwd())\n",
    "print(f\"Project root found at: {PROJECT_ROOT}\")\n",
    "# Now define the dataset path\n",
    "DATASET_PATH = PROJECT_ROOT / \"datasets\" / \"test_fqi.csv\"\n",
    "print(f\"Loading dataset from: {DATASET_PATH}\")\n",
    "\n",
    "from fogas_torch import PolicySolver, EnvDataCollector\n",
    "from fogas_torch.algorithm import (\n",
    "    FOGASSolverVectorized,\n",
    "    FOGASOracleSolverVectorized,\n",
    "    FOGASHyperOptimizer,\n",
    "    FOGASEvaluator,\n",
    ")\n",
    "from fogas.dataset_collection.dataset_analyzer import DatasetAnalyzer\n",
    "from fogas_torch.fqi.fqi_solver import FQISolver\n",
    "from fogas_torch.fqi.fqi_evaluator import FQIEvaluator\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed) # Add this\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7700872",
   "metadata": {},
   "source": [
    "## 3 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dedb2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = torch.arange(9)\n",
    "actions = torch.arange(4)\n",
    "N = len(states)  # number of states\n",
    "A = len(actions) # number of actions\n",
    "gamma = 0.9\n",
    "\n",
    "x_0 = 0 # fixed initial state\n",
    "\n",
    "goal_grid = 8   # absorbing terminal state\n",
    "\n",
    "def phi(x, a):\n",
    "    # Convert to scalar if tensor\n",
    "    if isinstance(x, torch.Tensor): x = x.item()\n",
    "    if isinstance(a, torch.Tensor): a = a.item()\n",
    "    \n",
    "    vec = torch.zeros(N * A, dtype=torch.float64)\n",
    "    vec[x * A + a] = 1.0\n",
    "    return vec\n",
    "\n",
    "omega = torch.zeros(N * A, dtype=torch.float64)\n",
    "omega[8 * A : 8 * A + 4] = 1.0\n",
    "\n",
    "# Helper to convert index <-> (row, col)\n",
    "def to_rc(s): \n",
    "    if isinstance(s, torch.Tensor): s = s.item()\n",
    "    return divmod(s, 3)\n",
    "\n",
    "def to_s(r, c): \n",
    "    return r*3 + c\n",
    "\n",
    "def next_state(s, a):\n",
    "    if isinstance(s, torch.Tensor): s = s.item()\n",
    "    if isinstance(a, torch.Tensor): a = a.item()\n",
    "\n",
    "    if s == goal_grid:\n",
    "        return goal_grid  # absorbing\n",
    "\n",
    "    r, c = to_rc(s)\n",
    "\n",
    "    if a == 0:  # Up\n",
    "        r = max(0, r-1)\n",
    "    elif a == 1:  # Down\n",
    "        r = min(2, r+1)\n",
    "    elif a == 2:  # Left\n",
    "        c = max(0, c-1)\n",
    "    elif a == 3:  # Right\n",
    "        c = min(2, c+1)\n",
    "\n",
    "    return to_s(r, c)\n",
    "\n",
    "def psi(xp):\n",
    "    if isinstance(xp, torch.Tensor): xp = xp.item()\n",
    "    \n",
    "    v = torch.zeros(N * A, dtype=torch.float64)\n",
    "    # Iterating over tensors (states/actions) yields 0-d tensors, so we use .item()\n",
    "    for x in states:\n",
    "        for a in actions:\n",
    "            if next_state(x, a) == xp:\n",
    "                idx = x.item() * A + a.item()\n",
    "                v[idx] = 1.0\n",
    "    return v\n",
    "\n",
    "# Initialize the solver\n",
    "mdp = PolicySolver(states=states, actions=actions, phi=phi, omega=omega, gamma=gamma, x0=x_0, psi=psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d572326b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FQI: 100%|████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 3503.25it/s, theta_norm=47.6510]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FQI Policy:\n",
      "\n",
      "========== LEARNED POLICY (FQI) ==========\n",
      "  State 0: π(a=0|s=0) = 0.00  π(a=1|s=0) = 0.00  π(a=2|s=0) = 0.00  π(a=3|s=0) = 1.00  --> best action: 3\n",
      "  State 1: π(a=0|s=1) = 0.00  π(a=1|s=1) = 1.00  π(a=2|s=1) = 0.00  π(a=3|s=1) = 0.00  --> best action: 1\n",
      "  State 2: π(a=0|s=2) = 0.00  π(a=1|s=2) = 1.00  π(a=2|s=2) = 0.00  π(a=3|s=2) = 0.00  --> best action: 1\n",
      "  State 3: π(a=0|s=3) = 0.00  π(a=1|s=3) = 0.00  π(a=2|s=3) = 0.00  π(a=3|s=3) = 1.00  --> best action: 3\n",
      "  State 4: π(a=0|s=4) = 0.00  π(a=1|s=4) = 0.00  π(a=2|s=4) = 0.00  π(a=3|s=4) = 1.00  --> best action: 3\n",
      "  State 5: π(a=0|s=5) = 0.00  π(a=1|s=5) = 1.00  π(a=2|s=5) = 0.00  π(a=3|s=5) = 0.00  --> best action: 1\n",
      "  State 6: π(a=0|s=6) = 0.00  π(a=1|s=6) = 0.00  π(a=2|s=6) = 0.00  π(a=3|s=6) = 1.00  --> best action: 3\n",
      "  State 7: π(a=0|s=7) = 0.00  π(a=1|s=7) = 0.00  π(a=2|s=7) = 0.00  π(a=3|s=7) = 1.00  --> best action: 3\n",
      "  State 8: π(a=0|s=8) = 1.00  π(a=1|s=8) = 0.00  π(a=2|s=8) = 0.00  π(a=3|s=8) = 0.00  --> best action: 0\n",
      "\n",
      "==========================================\n",
      "\n",
      "\n",
      "========== FINAL REWARD COMPARISON ==========\n",
      "\n",
      "J*(π*)   = 0.656100\n",
      "J(π_FQI) = 0.656100\n",
      "Gap (J* − J) = 0.000000e+00\n",
      "\n",
      "============================================\n",
      "\n",
      "\n",
      "========== OPTIMAL PATH (FQI Policy) ==========\n",
      "Step 0: State 0 -> Action 3 -> Reward 0.0000 -> Next 1\n",
      "Step 1: State 1 -> Action 1 -> Reward 0.0000 -> Next 4\n",
      "Step 2: State 4 -> Action 3 -> Reward 0.0000 -> Next 5\n",
      "Step 3: State 5 -> Action 1 -> Reward 0.0000 -> Next 8\n",
      "Step 4: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 5: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 6: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 7: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 8: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 9: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 10: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 11: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 12: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 13: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 14: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 15: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 16: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 17: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 18: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 19: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 20: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 21: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 22: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 23: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 24: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 25: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 26: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 27: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 28: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Step 29: State 8 -> Action 0 -> Reward 1.0000 -> Next 8\n",
      "Discounted Return (Simulated 30 steps): 6.1371\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize FQI Solver\n",
    "solver_fqi = FQISolver(\n",
    "    mdp=mdp,\n",
    "    csv_path=str(\"/shared/home/mauro.diaz/work/FOGAS/datasets/grid3_problem.csv\"),\n",
    "    device=device,\n",
    "    seed=seed,\n",
    "    ridge=1e-6 \n",
    ")\n",
    "evaluator_fqi = FQIEvaluator(solver_fqi)\n",
    "pi_fqi = solver_fqi.run(\n",
    "    K=1000, \n",
    "    tau=0.1, \n",
    "    verbose=True\n",
    ")\n",
    "print(\"\\nFQI Policy:\")\n",
    "evaluator_fqi.print_policy()\n",
    "evaluator_fqi.compare_final_rewards()\n",
    "evaluator_fqi.print_optimal_path(max_steps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ffa7d9",
   "metadata": {},
   "source": [
    "## 3 grid wall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f735cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = torch.arange(9)\n",
    "actions = torch.arange(4)\n",
    "N = len(states) # number of states\n",
    "A = len(actions) # number of actions\n",
    "gamma = 0.9\n",
    "\n",
    "x_0 = 0 # fixed initial state\n",
    "\n",
    "goal = 8   # absorbing terminal state\n",
    "pit = 5    # absorbing terminal state\n",
    "\n",
    "def phi(x, a):\n",
    "    # Convert to scalar if tensor\n",
    "    if isinstance(x, torch.Tensor): x = x.item()\n",
    "    if isinstance(a, torch.Tensor): a = a.item()\n",
    "\n",
    "    vec = torch.zeros(N * A, dtype=torch.float64)\n",
    "    vec[x * A + a] = 1.0\n",
    "    return vec\n",
    "\n",
    "step_cost = -0.1\n",
    "goal_reward = 1.0\n",
    "pit_reward  = -5.0\n",
    "\n",
    "omega = torch.full((N * A,), step_cost, dtype=torch.float64)\n",
    "\n",
    "# terminals: override step cost\n",
    "omega[goal * A : goal * A + A] = goal_reward\n",
    "omega[pit  * A : pit  * A + A] = pit_reward\n",
    "\n",
    "# Helper to convert index <-> (row, col)\n",
    "def to_rc(s):\n",
    "    if isinstance(s, torch.Tensor): s = s.item()\n",
    "    return divmod(s, 3)\n",
    "\n",
    "def to_s(r, c): \n",
    "    return r*3 + c\n",
    "\n",
    "wall = 4\n",
    "\n",
    "def next_state(s, a):\n",
    "    if isinstance(s, torch.Tensor): s = s.item()\n",
    "    if isinstance(a, torch.Tensor): a = a.item()\n",
    "\n",
    "    # absorbing terminals\n",
    "    if s == goal or s == pit:\n",
    "        return s\n",
    "\n",
    "    r, c = to_rc(s)\n",
    "\n",
    "    if a == 0:      # Up\n",
    "        r2, c2 = max(0, r-1), c\n",
    "    elif a == 1:    # Down\n",
    "        r2, c2 = min(2, r+1), c\n",
    "    elif a == 2:    # Left\n",
    "        r2, c2 = r, max(0, c-1)\n",
    "    elif a == 3:    # Right\n",
    "        r2, c2 = r, min(2, c+1)\n",
    "\n",
    "    sp = to_s(r2, c2)\n",
    "\n",
    "    # wall blocks transition\n",
    "    if sp == wall:\n",
    "        return s\n",
    "\n",
    "    return sp\n",
    "\n",
    "def psi(xp):\n",
    "    if isinstance(xp, torch.Tensor): xp = xp.item()\n",
    "    \n",
    "    v = torch.zeros(N * A, dtype=torch.float64)\n",
    "    for x in states:\n",
    "        for a in actions:\n",
    "            if next_state(x, a) == xp:\n",
    "                idx = x.item() * A + a.item()\n",
    "                v[idx] = 1.0\n",
    "    return v\n",
    "\n",
    "# Initialize the solver with explicit terminal states\n",
    "mdp = PolicySolver(\n",
    "    states=states, \n",
    "    actions=actions, \n",
    "    phi=phi, \n",
    "    omega=omega, \n",
    "    gamma=gamma, \n",
    "    x0=x_0, \n",
    "    psi=psi,\n",
    "    terminal_states=[goal, pit] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f7118d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset saved to: /shared/home/mauro.diaz/work/FOGAS/datasets/test_fqi.csv\n",
      "   Total transitions: 3000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>step</th>\n",
       "      <th>state</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>next_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>125</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>125</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>125</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>125</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>125</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      episode  step  state  action  reward  next_state\n",
       "0           0     0      0       0    -0.1           0\n",
       "1           0     1      0       3    -0.1           1\n",
       "2           0     2      1       2    -0.1           0\n",
       "3           0     3      0       1    -0.1           3\n",
       "4           0     4      3       1    -0.1           6\n",
       "...       ...   ...    ...     ...     ...         ...\n",
       "2995      125     5      2       2    -0.1           1\n",
       "2996      125     6      1       1    -0.1           1\n",
       "2997      125     7      1       1    -0.1           1\n",
       "2998      125     8      1       1    -0.1           1\n",
       "2999      125     9      1       2    -0.1           0\n",
       "\n",
       "[3000 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collector = EnvDataCollector(mdp=mdp, env_name=\"3grid_wall\", max_steps=50)\n",
    "collector.collect_dataset(n_steps=3000, save_path=str(DATASET_PATH), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f882db51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FQI: 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 4107.45it/s, theta_norm=1.3305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FQI Policy:\n",
      "\n",
      "========== LEARNED POLICY (FQI) ==========\n",
      "  State 0: π(a=0|s=0) = 0.00  π(a=1|s=0) = 0.00  π(a=2|s=0) = 0.00  π(a=3|s=0) = 1.00  --> best action: 3\n",
      "  State 1: π(a=0|s=1) = 0.00  π(a=1|s=1) = 0.00  π(a=2|s=1) = 0.00  π(a=3|s=1) = 1.00  --> best action: 3\n",
      "  State 2: π(a=0|s=2) = 0.00  π(a=1|s=2) = 1.00  π(a=2|s=2) = 0.00  π(a=3|s=2) = 0.00  --> best action: 1\n",
      "  State 3: π(a=0|s=3) = 0.00  π(a=1|s=3) = 1.00  π(a=2|s=3) = 0.00  π(a=3|s=3) = 0.00  --> best action: 1\n",
      "  State 4: π(a=0|s=4) = 1.00  π(a=1|s=4) = 0.00  π(a=2|s=4) = 0.00  π(a=3|s=4) = 0.00  --> best action: 0\n",
      "  State 5: π(a=0|s=5) = 1.00  π(a=1|s=5) = 0.00  π(a=2|s=5) = 0.00  π(a=3|s=5) = 0.00  --> best action: 0\n",
      "  State 6: π(a=0|s=6) = 0.00  π(a=1|s=6) = 0.00  π(a=2|s=6) = 0.00  π(a=3|s=6) = 1.00  --> best action: 3\n",
      "  State 7: π(a=0|s=7) = 0.00  π(a=1|s=7) = 0.00  π(a=2|s=7) = 0.00  π(a=3|s=7) = 1.00  --> best action: 3\n",
      "  State 8: π(a=0|s=8) = 1.00  π(a=1|s=8) = 0.00  π(a=2|s=8) = 0.00  π(a=3|s=8) = 0.00  --> best action: 0\n",
      "\n",
      "==========================================\n",
      "\n",
      "\n",
      "========== FINAL REWARD COMPARISON ==========\n",
      "\n",
      "J*(π*)   = 0.621710\n",
      "J(π_FQI) = -3.672100\n",
      "Gap (J* − J) = 4.293810e+00\n",
      "\n",
      "============================================\n",
      "\n",
      "\n",
      "========== OPTIMAL PATH (FQI Policy) ==========\n",
      "Step 0: State 0 -> Action 3 -> Reward -0.1000 -> Next 1\n",
      "Step 1: State 1 -> Action 3 -> Reward -0.1000 -> Next 2\n",
      "Step 2: State 2 -> Action 1 -> Reward -0.1000 -> Next 5\n",
      "Step 3: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 4: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 5: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 6: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 7: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 8: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 9: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 10: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 11: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 12: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 13: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 14: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 15: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 16: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 17: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 18: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 19: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 20: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 21: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 22: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 23: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 24: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 25: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 26: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 27: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 28: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 29: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Discounted Return (Simulated 30 steps): -34.6014\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "solver_fqi = FQISolver(\n",
    "    mdp=mdp,\n",
    "    csv_path=str(DATASET_PATH),\n",
    "    device=device,\n",
    "    seed=seed,\n",
    "    ridge=1e-6 \n",
    ")\n",
    "evaluator_fqi = FQIEvaluator(solver_fqi)\n",
    "pi_fqi = solver_fqi.run(\n",
    "    K=1000, \n",
    "    tau=0.3, \n",
    "    verbose=True\n",
    ")\n",
    "print(\"\\nFQI Policy:\")\n",
    "evaluator_fqi.print_policy()\n",
    "evaluator_fqi.compare_final_rewards()\n",
    "evaluator_fqi.print_optimal_path(max_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c06527d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Terminal-aware dataset saved to: /shared/home/mauro.diaz/work/FOGAS/datasets/test_fqi.csv\n",
      "   Total transitions: 1000\n"
     ]
    }
   ],
   "source": [
    "df = collector.collect_dataset_terminal_aware(\n",
    "    policy=\"random\", \n",
    "    n_steps=1000, \n",
    "    extra_steps=5,\n",
    "    save_path=str(DATASET_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b7ed91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FQI: 100%|█████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 3970.47it/s, theta_norm=1.3305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FQI Policy:\n",
      "\n",
      "========== LEARNED POLICY (FQI) ==========\n",
      "  State 0: π(a=0|s=0) = 0.00  π(a=1|s=0) = 0.00  π(a=2|s=0) = 0.00  π(a=3|s=0) = 1.00  --> best action: 3\n",
      "  State 1: π(a=0|s=1) = 0.00  π(a=1|s=1) = 0.00  π(a=2|s=1) = 0.00  π(a=3|s=1) = 1.00  --> best action: 3\n",
      "  State 2: π(a=0|s=2) = 0.00  π(a=1|s=2) = 1.00  π(a=2|s=2) = 0.00  π(a=3|s=2) = 0.00  --> best action: 1\n",
      "  State 3: π(a=0|s=3) = 0.00  π(a=1|s=3) = 1.00  π(a=2|s=3) = 0.00  π(a=3|s=3) = 0.00  --> best action: 1\n",
      "  State 4: π(a=0|s=4) = 1.00  π(a=1|s=4) = 0.00  π(a=2|s=4) = 0.00  π(a=3|s=4) = 0.00  --> best action: 0\n",
      "  State 5: π(a=0|s=5) = 1.00  π(a=1|s=5) = 0.00  π(a=2|s=5) = 0.00  π(a=3|s=5) = 0.00  --> best action: 0\n",
      "  State 6: π(a=0|s=6) = 0.00  π(a=1|s=6) = 0.00  π(a=2|s=6) = 0.00  π(a=3|s=6) = 1.00  --> best action: 3\n",
      "  State 7: π(a=0|s=7) = 0.00  π(a=1|s=7) = 0.00  π(a=2|s=7) = 0.00  π(a=3|s=7) = 1.00  --> best action: 3\n",
      "  State 8: π(a=0|s=8) = 1.00  π(a=1|s=8) = 0.00  π(a=2|s=8) = 0.00  π(a=3|s=8) = 0.00  --> best action: 0\n",
      "\n",
      "==========================================\n",
      "\n",
      "\n",
      "========== FINAL REWARD COMPARISON ==========\n",
      "\n",
      "J*(π*)   = 0.621710\n",
      "J(π_FQI) = -3.672100\n",
      "Gap (J* − J) = 4.293810e+00\n",
      "\n",
      "============================================\n",
      "\n",
      "\n",
      "========== OPTIMAL PATH (FQI Policy) ==========\n",
      "Step 0: State 0 -> Action 3 -> Reward -0.1000 -> Next 1\n",
      "Step 1: State 1 -> Action 3 -> Reward -0.1000 -> Next 2\n",
      "Step 2: State 2 -> Action 1 -> Reward -0.1000 -> Next 5\n",
      "Step 3: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 4: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 5: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 6: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 7: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 8: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 9: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 10: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 11: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 12: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 13: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 14: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 15: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 16: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 17: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 18: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 19: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 20: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 21: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 22: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 23: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 24: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 25: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 26: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 27: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 28: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Step 29: State 5 -> Action 0 -> Reward -5.0000 -> Next 5\n",
      "Discounted Return (Simulated 30 steps): -34.6014\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize FQI Solver\n",
    "solver_fqi = FQISolver(\n",
    "    mdp=mdp,\n",
    "    csv_path=str(DATASET_PATH),\n",
    "    device=device,\n",
    "    seed=seed,\n",
    "    ridge=1e-6 \n",
    ")\n",
    "evaluator_fqi = FQIEvaluator(solver_fqi)\n",
    "pi_fqi = solver_fqi.run(\n",
    "    K=2000, \n",
    "    tau=0.0, \n",
    "    verbose=True,\n",
    "\n",
    ")\n",
    "print(\"\\nFQI Policy:\")\n",
    "evaluator_fqi.print_policy()\n",
    "evaluator_fqi.compare_final_rewards()\n",
    "evaluator_fqi.print_optimal_path(max_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d70b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FOGAS (Python 3.10)",
   "language": "python",
   "name": "fogas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
