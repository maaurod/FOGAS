{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d559d62",
   "metadata": {},
   "source": [
    "# Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4028b00f",
   "metadata": {},
   "source": [
    "**10×10 Gridworld**\n",
    "\n",
    "**Layout (row-major indices)**\n",
    "\n",
    "Legend: **S** start, **G** goal (terminal), **P** pit (terminal), **#** wall, **.** empty\n",
    "\n",
    "```\n",
    "r\\c  0 1 2 3 4 5 6 7 8 9\n",
    "0    S . . . # . . . . .\n",
    "1    . # . . # . . # P .\n",
    "2    . # # . . . . # . .\n",
    "3    . . P . # . . # . .\n",
    "4    # . # # # # # # . #\n",
    "5    . . . . # . . P . .\n",
    "6    . P # . # . # . . .\n",
    "7    . . # . . P # . . .\n",
    "8    . . # . # . # # . .\n",
    "9    . . . . # . . . . G\n",
    "```\n",
    "\n",
    "**State space**\n",
    "\n",
    "$$\n",
    "\\mathcal{X}={0,1,\\dots,99},\\quad x=10r+c\n",
    "$$\n",
    "\n",
    "* **Goal (absorbing):** $x_G=99$\n",
    "* **Pits (absorbing):**\n",
    "  $\n",
    "  \\mathcal{X}_{\\text{pit}}={18,32,57,61,75,88}\n",
    "  $\n",
    "* **Walls (impassable):**\n",
    "  $\n",
    "  \\mathcal{X}_{\\text{wall}}={4,11,14,17,21,22,27,34,37,40,42,43,44,45,46,47,49,54,62,64,66,72,76,82,84,86,87,94}\n",
    "  $\n",
    "\n",
    "Terminal set:\n",
    "$\n",
    "\\mathcal{T}={99}\\cup \\mathcal{X}_{\\text{pit}}\n",
    "$\n",
    "\n",
    "**Action space**\n",
    "\n",
    "$$\n",
    "\\mathcal{A}={0,1,2,3}\n",
    "$$\n",
    "where (0=) Up, (1=) Down, (2=) Left, (3=) Right.\n",
    "\n",
    "**Transition dynamics (deterministic)**\n",
    "\n",
    "Let $x\\mapsto(r,c)$ by $r=\\lfloor x/10\\rfloor,\\ c=x\\bmod 10$.\n",
    "\n",
    "* If $x\\in\\mathcal{T}$, then $\\text{next\\_state}(x,a)=x$ $absorbing$.\n",
    "* Otherwise, compute the intended neighbor $(r',c')$ from action $a$.\n",
    "* If $(r',c')$ is outside the grid, **stay**.\n",
    "* Let $x'=10r'+c'$. If $x'\\in\\mathcal{X}_{\\text{wall}}$, **stay**.\n",
    "\n",
    "Transition matrix shape:\n",
    "$$\n",
    "P\\in\\mathbb{R}^{(|\\mathcal{X}||\\mathcal{A}|)\\times|\\mathcal{X}|}=\\mathbb{R}^{400\\times 100},\n",
    "$$\n",
    "with\n",
    "$$\n",
    "P[(x,a),x']=1 \\iff x'=\\text{next\\_state}(x,a)\n",
    "$$\n",
    "\n",
    "**Reward model (standard step cost + terminal outcomes)**\n",
    "\n",
    "Reward based on the **next state**:\n",
    "$$\n",
    "r(x,a)=\n",
    "\\begin{cases}\n",
    "+1 & \\text{if } \\text{next\\_state}(x,a)=99,\\\n",
    "-5 & \\text{if } \\text{next\\_state}(x,a)\\in \\mathcal{X}_{\\text{pit}},\\\n",
    "-0.1 & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Initial state**\n",
    "\n",
    "$$\n",
    "x_0=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "315b375a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve().parent  # go to repo root\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Define dataset path\n",
    "DATASET_PATH = PROJECT_ROOT / \"datasets\" / \"10grid_wall.csv\"\n",
    "DATASET_PATH2 = PROJECT_ROOT / \"datasets\" / \"10grid_wall2.csv\"\n",
    "\n",
    "from fogas_torch import PolicySolver, EnvDataCollector\n",
    "from fogas_torch.algorithm import (\n",
    "    FOGASSolverVectorized,\n",
    "    FOGASOracleSolverVectorized,\n",
    "    FOGASHyperOptimizer,\n",
    "    FOGASEvaluator,\n",
    ")\n",
    "from fogas.dataset_collection.dataset_analyzer import DatasetAnalyzer\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed) # Add this\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a2e081",
   "metadata": {},
   "source": [
    "# Tabular Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75936a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 10x10 Four-Rooms Grid =====\n",
    "\n",
    "states  = torch.arange(100, dtype=torch.int64)\n",
    "actions = torch.arange(4, dtype=torch.int64)\n",
    "N = len(states)     # 100\n",
    "A = len(actions)    # 4\n",
    "gamma = 0.9\n",
    "\n",
    "x_0 = 0  # fixed initial state\n",
    "\n",
    "goal = 99  # absorbing terminal state\n",
    "\n",
    "# pits (absorbing terminal states)\n",
    "pits = {18, 32, 57, 61, 75}\n",
    "\n",
    "# walls (impassable)\n",
    "walls = {\n",
    "    4, 11, 14, 17, 21, 22, 27, 34, 37,\n",
    "    40, 42, 43, 44, 45, 46, 47, 49,\n",
    "    54, 62, 64, 66, 72, 76, 82, 84, 86, 87, 94\n",
    "}\n",
    "\n",
    "# ---------- Tabular features ----------\n",
    "def phi(x, a):\n",
    "    vec = torch.zeros(N * A, dtype=torch.float64)\n",
    "    vec[int(x) * A + int(a)] = 1.0\n",
    "    return vec\n",
    "\n",
    "# ---------- Reward weights (tabular) ----------\n",
    "step_cost = -0.1\n",
    "goal_reward = 1.0\n",
    "pit_reward  = -5.0\n",
    "\n",
    "omega = torch.full((N * A,), step_cost, dtype=torch.float64)\n",
    "\n",
    "# override terminal rewards (all actions in terminal states)\n",
    "omega[goal * A : goal * A + A] = goal_reward\n",
    "for p in pits:\n",
    "    omega[p * A : p * A + A] = pit_reward\n",
    "\n",
    "# ---------- Helpers: index <-> (row, col) ----------\n",
    "def to_rc(s):  return divmod(s, 10)\n",
    "def to_s(r, c): return r * 10 + c\n",
    "\n",
    "# ---------- Dynamics ----------\n",
    "def next_state(s, a):\n",
    "    # absorbing terminals\n",
    "    if s == goal or s in pits:\n",
    "        return s\n",
    "\n",
    "    r, c = to_rc(s)\n",
    "\n",
    "    if a == 0:      # Up\n",
    "        r2, c2 = max(0, r - 1), c\n",
    "    elif a == 1:    # Down\n",
    "        r2, c2 = min(9, r + 1), c\n",
    "    elif a == 2:    # Left\n",
    "        r2, c2 = r, max(0, c - 1)\n",
    "    elif a == 3:    # Right\n",
    "        r2, c2 = r, min(9, c + 1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid action\")\n",
    "\n",
    "    sp = to_s(r2, c2)\n",
    "\n",
    "    # wall blocks transition\n",
    "    if sp in walls:\n",
    "        return s\n",
    "\n",
    "    return sp\n",
    "\n",
    "# ---------- Backward feature map psi(x') ----------\n",
    "def psi(xp):\n",
    "    v = torch.zeros(N * A, dtype=torch.float64)\n",
    "    for x in states:\n",
    "        for a in actions:\n",
    "            if next_state(int(x), int(a)) == xp:\n",
    "                v[int(x) * A + int(a)] = 1.0\n",
    "    return v\n",
    "\n",
    "# ---------- Build solver ----------\n",
    "mdp = PolicySolver(\n",
    "    states=states,\n",
    "    actions=actions,\n",
    "    phi=phi,\n",
    "    omega=omega,\n",
    "    gamma=gamma,\n",
    "    x0=x_0,\n",
    "    psi=psi\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ca7d28",
   "metadata": {},
   "source": [
    "## Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d76cb547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ FOGAS PARAMETER SUMMARY ================\n",
      "\n",
      "Basic Information\n",
      "-----------------\n",
      "Dataset size n:           10000000.0\n",
      "Feature norm bound R:     1.0000\n",
      "Num states N:             100\n",
      "Num actions A:            4\n",
      "Feature dim d:            400\n",
      "Discount γ:               0.9\n",
      "Confidence δ:             0.05\n",
      "\n",
      "Theoretical Quantities\n",
      "----------------------\n",
      "T_min (theoretical):      9255128.526390366\n",
      "T (iterations):                1000   (overridden → 1000)\n",
      "\n",
      "FOGAS Hyperparameters\n",
      "---------------------\n",
      "alpha:                        0.000263\n",
      "rho:                            66.292053\n",
      "eta:                            0.000002\n",
      "D_theta:                    63.245553\n",
      "beta (ridge):             0.000003\n",
      "D_pi (derived):           16.651092\n",
      "\n",
      "=========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "solver = FOGASOracleSolverVectorized(\n",
    "    mdp=mdp, \n",
    "    cov_matrix=\"identity\", \n",
    "    print_params=True, \n",
    "    T=1000,\n",
    "    device=device,\n",
    "    seed=seed\n",
    ")\n",
    "evaluator = FOGASEvaluator(solver)\n",
    "optimizer = FOGASHyperOptimizer(solver=solver,metric_callable=evaluator.get_metric(\"reward\"), seed=seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107c2404",
   "metadata": {},
   "source": [
    "We do an hyperparameter optimization because is not clear how will work the Oracle in a big dimensional problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51219d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FOGAS Hyperparameter Optimization ===\n",
      "[Theory] metric = 0.3761\n",
      "Order: ('alpha', 'rho', 'eta')\n",
      "\n",
      "Optimizing alpha\n",
      "[After alpha*] alpha=4.7512e+00, rho=1.0000e-02, eta=1.5215e-06 | metric = 0.1000\n",
      "\n",
      "Optimizing rho\n",
      "[After rho*] alpha=4.7512e+00, rho=1.0000e-02, eta=1.5215e-06 | metric = 0.1000\n",
      "\n",
      "Optimizing eta\n",
      "[After eta*] alpha=4.7512e+00, rho=1.0000e-02, eta=3.0000e+00 | metric = 0.0432\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary = optimizer.optimize_fogas_hyperparameters(\n",
    "    search_method=\"random\",\n",
    "    coarse_points=7,\n",
    "    print_main=True, \n",
    "    num_runs=1,\n",
    "    random_candidates=10,\n",
    "    order=(\"alpha\", \"rho\", \"eta\"),\n",
    "    bounds_overrides={\"alpha\" : (1e-2, 5.0), \"rho\" : (1e-2, 3.0), \"eta\" : (1e-2, 3.0)}\n",
    ")\n",
    "\n",
    "alpha_star = summary[\"alpha\"]\n",
    "rho_star = summary[\"rho\"]\n",
    "eta_star = summary[\"eta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac95bb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOGAS Oracle: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:54<00:00, 185.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  State 0: π(a=0|s=0) = 0.00  π(a=1|s=0) = 0.41  π(a=2|s=0) = 0.00  π(a=3|s=0) = 0.59  --> best action: 3\n",
      "  State 1: π(a=0|s=1) = 0.00  π(a=1|s=1) = 0.00  π(a=2|s=1) = 0.00  π(a=3|s=1) = 1.00  --> best action: 3\n",
      "  State 2: π(a=0|s=2) = 0.00  π(a=1|s=2) = 0.50  π(a=2|s=2) = 0.00  π(a=3|s=2) = 0.50  --> best action: 1\n",
      "  State 3: π(a=0|s=3) = 0.00  π(a=1|s=3) = 1.00  π(a=2|s=3) = 0.00  π(a=3|s=3) = 0.00  --> best action: 1\n",
      "  State 4: π(a=0|s=4) = 0.00  π(a=1|s=4) = 0.00  π(a=2|s=4) = 0.00  π(a=3|s=4) = 1.00  --> best action: 3\n",
      "  State 5: π(a=0|s=5) = 0.00  π(a=1|s=5) = 0.00  π(a=2|s=5) = 0.00  π(a=3|s=5) = 1.00  --> best action: 3\n",
      "  State 6: π(a=0|s=6) = 0.00  π(a=1|s=6) = 0.00  π(a=2|s=6) = 0.00  π(a=3|s=6) = 1.00  --> best action: 3\n",
      "  State 7: π(a=0|s=7) = 0.00  π(a=1|s=7) = 0.00  π(a=2|s=7) = 0.00  π(a=3|s=7) = 1.00  --> best action: 3\n",
      "  State 8: π(a=0|s=8) = 0.00  π(a=1|s=8) = 0.00  π(a=2|s=8) = 0.00  π(a=3|s=8) = 1.00  --> best action: 3\n",
      "  State 9: π(a=0|s=9) = 0.00  π(a=1|s=9) = 1.00  π(a=2|s=9) = 0.00  π(a=3|s=9) = 0.00  --> best action: 1\n",
      "  State 10: π(a=0|s=10) = 0.00  π(a=1|s=10) = 1.00  π(a=2|s=10) = 0.00  π(a=3|s=10) = 0.00  --> best action: 1\n",
      "  State 11: π(a=0|s=11) = 0.00  π(a=1|s=11) = 0.00  π(a=2|s=11) = 0.00  π(a=3|s=11) = 1.00  --> best action: 3\n",
      "  State 12: π(a=0|s=12) = 0.00  π(a=1|s=12) = 0.00  π(a=2|s=12) = 0.00  π(a=3|s=12) = 1.00  --> best action: 3\n",
      "  State 13: π(a=0|s=13) = 0.00  π(a=1|s=13) = 1.00  π(a=2|s=13) = 0.00  π(a=3|s=13) = 0.00  --> best action: 1\n",
      "  State 14: π(a=0|s=14) = 0.00  π(a=1|s=14) = 0.00  π(a=2|s=14) = 0.00  π(a=3|s=14) = 1.00  --> best action: 3\n",
      "  State 15: π(a=0|s=15) = 0.64  π(a=1|s=15) = 0.00  π(a=2|s=15) = 0.00  π(a=3|s=15) = 0.36  --> best action: 0\n",
      "  State 16: π(a=0|s=16) = 1.00  π(a=1|s=16) = 0.00  π(a=2|s=16) = 0.00  π(a=3|s=16) = 0.00  --> best action: 0\n",
      "  State 17: π(a=0|s=17) = 1.00  π(a=1|s=17) = 0.00  π(a=2|s=17) = 0.00  π(a=3|s=17) = 0.00  --> best action: 0\n",
      "  State 18: π(a=0|s=18) = 0.25  π(a=1|s=18) = 0.25  π(a=2|s=18) = 0.25  π(a=3|s=18) = 0.25  --> best action: 0\n",
      "  State 19: π(a=0|s=19) = 0.00  π(a=1|s=19) = 1.00  π(a=2|s=19) = 0.00  π(a=3|s=19) = 0.00  --> best action: 1\n",
      "  State 20: π(a=0|s=20) = 0.00  π(a=1|s=20) = 1.00  π(a=2|s=20) = 0.00  π(a=3|s=20) = 0.00  --> best action: 1\n",
      "  State 21: π(a=0|s=21) = 0.00  π(a=1|s=21) = 1.00  π(a=2|s=21) = 0.00  π(a=3|s=21) = 0.00  --> best action: 1\n",
      "  State 22: π(a=0|s=22) = 0.00  π(a=1|s=22) = 0.00  π(a=2|s=22) = 0.00  π(a=3|s=22) = 1.00  --> best action: 3\n",
      "  State 23: π(a=0|s=23) = 0.00  π(a=1|s=23) = 0.00  π(a=2|s=23) = 0.00  π(a=3|s=23) = 1.00  --> best action: 3\n",
      "  State 24: π(a=0|s=24) = 0.00  π(a=1|s=24) = 0.00  π(a=2|s=24) = 0.00  π(a=3|s=24) = 1.00  --> best action: 3\n",
      "  State 25: π(a=0|s=25) = 0.57  π(a=1|s=25) = 0.00  π(a=2|s=25) = 0.00  π(a=3|s=25) = 0.43  --> best action: 0\n",
      "  State 26: π(a=0|s=26) = 1.00  π(a=1|s=26) = 0.00  π(a=2|s=26) = 0.00  π(a=3|s=26) = 0.00  --> best action: 0\n",
      "  State 27: π(a=0|s=27) = 0.00  π(a=1|s=27) = 0.00  π(a=2|s=27) = 0.00  π(a=3|s=27) = 1.00  --> best action: 3\n",
      "  State 28: π(a=0|s=28) = 0.00  π(a=1|s=28) = 1.00  π(a=2|s=28) = 0.00  π(a=3|s=28) = 0.00  --> best action: 1\n",
      "  State 29: π(a=0|s=29) = 0.00  π(a=1|s=29) = 0.86  π(a=2|s=29) = 0.14  π(a=3|s=29) = 0.00  --> best action: 1\n",
      "  State 30: π(a=0|s=30) = 0.00  π(a=1|s=30) = 0.00  π(a=2|s=30) = 0.00  π(a=3|s=30) = 0.99  --> best action: 3\n",
      "  State 31: π(a=0|s=31) = 0.13  π(a=1|s=31) = 0.87  π(a=2|s=31) = 0.00  π(a=3|s=31) = 0.00  --> best action: 1\n",
      "  State 32: π(a=0|s=32) = 0.25  π(a=1|s=32) = 0.25  π(a=2|s=32) = 0.25  π(a=3|s=32) = 0.25  --> best action: 0\n",
      "  State 33: π(a=0|s=33) = 1.00  π(a=1|s=33) = 0.00  π(a=2|s=33) = 0.00  π(a=3|s=33) = 0.00  --> best action: 0\n",
      "  State 34: π(a=0|s=34) = 0.00  π(a=1|s=34) = 0.00  π(a=2|s=34) = 0.00  π(a=3|s=34) = 1.00  --> best action: 3\n",
      "  State 35: π(a=0|s=35) = 0.08  π(a=1|s=35) = 0.00  π(a=2|s=35) = 0.00  π(a=3|s=35) = 0.92  --> best action: 3\n",
      "  State 36: π(a=0|s=36) = 1.00  π(a=1|s=36) = 0.00  π(a=2|s=36) = 0.00  π(a=3|s=36) = 0.00  --> best action: 0\n",
      "  State 37: π(a=0|s=37) = 0.00  π(a=1|s=37) = 0.00  π(a=2|s=37) = 0.00  π(a=3|s=37) = 1.00  --> best action: 3\n",
      "  State 38: π(a=0|s=38) = 0.00  π(a=1|s=38) = 1.00  π(a=2|s=38) = 0.00  π(a=3|s=38) = 0.00  --> best action: 1\n",
      "  State 39: π(a=0|s=39) = 0.00  π(a=1|s=39) = 0.00  π(a=2|s=39) = 1.00  π(a=3|s=39) = 0.00  --> best action: 2\n",
      "  State 40: π(a=0|s=40) = 0.00  π(a=1|s=40) = 1.00  π(a=2|s=40) = 0.00  π(a=3|s=40) = 0.00  --> best action: 1\n",
      "  State 41: π(a=0|s=41) = 0.00  π(a=1|s=41) = 0.47  π(a=2|s=41) = 0.26  π(a=3|s=41) = 0.26  --> best action: 1\n",
      "  State 42: π(a=0|s=42) = 0.00  π(a=1|s=42) = 1.00  π(a=2|s=42) = 0.00  π(a=3|s=42) = 0.00  --> best action: 1\n",
      "  State 43: π(a=0|s=43) = 0.89  π(a=1|s=43) = 0.00  π(a=2|s=43) = 0.06  π(a=3|s=43) = 0.06  --> best action: 0\n",
      "  State 44: π(a=0|s=44) = 0.25  π(a=1|s=44) = 0.25  π(a=2|s=44) = 0.25  π(a=3|s=44) = 0.25  --> best action: 0\n",
      "  State 45: π(a=0|s=45) = 1.00  π(a=1|s=45) = 0.00  π(a=2|s=45) = 0.00  π(a=3|s=45) = 0.00  --> best action: 0\n",
      "  State 46: π(a=0|s=46) = 1.00  π(a=1|s=46) = 0.00  π(a=2|s=46) = 0.00  π(a=3|s=46) = 0.00  --> best action: 0\n",
      "  State 47: π(a=0|s=47) = 0.00  π(a=1|s=47) = 0.00  π(a=2|s=47) = 0.00  π(a=3|s=47) = 1.00  --> best action: 3\n",
      "  State 48: π(a=0|s=48) = 0.00  π(a=1|s=48) = 1.00  π(a=2|s=48) = 0.00  π(a=3|s=48) = 0.00  --> best action: 1\n",
      "  State 49: π(a=0|s=49) = 0.00  π(a=1|s=49) = 1.00  π(a=2|s=49) = 0.00  π(a=3|s=49) = 0.00  --> best action: 1\n",
      "  State 50: π(a=0|s=50) = 0.27  π(a=1|s=50) = 0.40  π(a=2|s=50) = 0.27  π(a=3|s=50) = 0.06  --> best action: 1\n",
      "  State 51: π(a=0|s=51) = 0.01  π(a=1|s=51) = 0.00  π(a=2|s=51) = 0.51  π(a=3|s=51) = 0.48  --> best action: 2\n",
      "  State 52: π(a=0|s=52) = 0.27  π(a=1|s=52) = 0.27  π(a=2|s=52) = 0.06  π(a=3|s=52) = 0.40  --> best action: 3\n",
      "  State 53: π(a=0|s=53) = 0.25  π(a=1|s=53) = 0.37  π(a=2|s=53) = 0.13  π(a=3|s=53) = 0.25  --> best action: 1\n",
      "  State 54: π(a=0|s=54) = 0.47  π(a=1|s=54) = 0.47  π(a=2|s=54) = 0.05  π(a=3|s=54) = 0.00  --> best action: 0\n",
      "  State 55: π(a=0|s=55) = 0.50  π(a=1|s=55) = 0.00  π(a=2|s=55) = 0.50  π(a=3|s=55) = 0.00  --> best action: 0\n",
      "  State 56: π(a=0|s=56) = 0.00  π(a=1|s=56) = 0.00  π(a=2|s=56) = 1.00  π(a=3|s=56) = 0.00  --> best action: 2\n",
      "  State 57: π(a=0|s=57) = 0.25  π(a=1|s=57) = 0.25  π(a=2|s=57) = 0.25  π(a=3|s=57) = 0.25  --> best action: 0\n",
      "  State 58: π(a=0|s=58) = 0.00  π(a=1|s=58) = 0.59  π(a=2|s=58) = 0.00  π(a=3|s=58) = 0.41  --> best action: 1\n",
      "  State 59: π(a=0|s=59) = 0.00  π(a=1|s=59) = 1.00  π(a=2|s=59) = 0.00  π(a=3|s=59) = 0.00  --> best action: 1\n",
      "  State 60: π(a=0|s=60) = 0.11  π(a=1|s=60) = 0.66  π(a=2|s=60) = 0.23  π(a=3|s=60) = 0.00  --> best action: 1\n",
      "  State 61: π(a=0|s=61) = 0.25  π(a=1|s=61) = 0.25  π(a=2|s=61) = 0.25  π(a=3|s=61) = 0.25  --> best action: 0\n",
      "  State 62: π(a=0|s=62) = 0.06  π(a=1|s=62) = 0.26  π(a=2|s=62) = 0.00  π(a=3|s=62) = 0.68  --> best action: 3\n",
      "  State 63: π(a=0|s=63) = 0.15  π(a=1|s=63) = 0.32  π(a=2|s=63) = 0.27  π(a=3|s=63) = 0.27  --> best action: 1\n",
      "  State 64: π(a=0|s=64) = 0.07  π(a=1|s=64) = 0.00  π(a=2|s=64) = 0.93  π(a=3|s=64) = 0.00  --> best action: 2\n",
      "  State 65: π(a=0|s=65) = 1.00  π(a=1|s=65) = 0.00  π(a=2|s=65) = 0.00  π(a=3|s=65) = 0.00  --> best action: 0\n",
      "  State 66: π(a=0|s=66) = 0.00  π(a=1|s=66) = 0.00  π(a=2|s=66) = 0.00  π(a=3|s=66) = 1.00  --> best action: 3\n",
      "  State 67: π(a=0|s=67) = 0.00  π(a=1|s=67) = 0.81  π(a=2|s=67) = 0.00  π(a=3|s=67) = 0.19  --> best action: 1\n",
      "  State 68: π(a=0|s=68) = 0.00  π(a=1|s=68) = 0.79  π(a=2|s=68) = 0.00  π(a=3|s=68) = 0.21  --> best action: 1\n",
      "  State 69: π(a=0|s=69) = 0.00  π(a=1|s=69) = 1.00  π(a=2|s=69) = 0.00  π(a=3|s=69) = 0.00  --> best action: 1\n",
      "  State 70: π(a=0|s=70) = 0.05  π(a=1|s=70) = 0.41  π(a=2|s=70) = 0.29  π(a=3|s=70) = 0.26  --> best action: 1\n",
      "  State 71: π(a=0|s=71) = 0.00  π(a=1|s=71) = 0.58  π(a=2|s=71) = 0.25  π(a=3|s=71) = 0.17  --> best action: 1\n",
      "  State 72: π(a=0|s=72) = 0.42  π(a=1|s=72) = 0.42  π(a=2|s=72) = 0.10  π(a=3|s=72) = 0.06  --> best action: 0\n",
      "  State 73: π(a=0|s=73) = 0.19  π(a=1|s=73) = 0.63  π(a=2|s=73) = 0.18  π(a=3|s=73) = 0.00  --> best action: 1\n",
      "  State 74: π(a=0|s=74) = 0.00  π(a=1|s=74) = 0.00  π(a=2|s=74) = 1.00  π(a=3|s=74) = 0.00  --> best action: 2\n",
      "  State 75: π(a=0|s=75) = 0.25  π(a=1|s=75) = 0.25  π(a=2|s=75) = 0.25  π(a=3|s=75) = 0.25  --> best action: 0\n",
      "  State 76: π(a=0|s=76) = 0.00  π(a=1|s=76) = 0.00  π(a=2|s=76) = 0.00  π(a=3|s=76) = 1.00  --> best action: 3\n",
      "  State 77: π(a=0|s=77) = 0.00  π(a=1|s=77) = 0.00  π(a=2|s=77) = 0.00  π(a=3|s=77) = 1.00  --> best action: 3\n",
      "  State 78: π(a=0|s=78) = 0.00  π(a=1|s=78) = 0.81  π(a=2|s=78) = 0.00  π(a=3|s=78) = 0.19  --> best action: 1\n",
      "  State 79: π(a=0|s=79) = 0.00  π(a=1|s=79) = 1.00  π(a=2|s=79) = 0.00  π(a=3|s=79) = 0.00  --> best action: 1\n",
      "  State 80: π(a=0|s=80) = 0.15  π(a=1|s=80) = 0.31  π(a=2|s=80) = 0.26  π(a=3|s=80) = 0.28  --> best action: 1\n",
      "  State 81: π(a=0|s=81) = 0.10  π(a=1|s=81) = 0.35  π(a=2|s=81) = 0.26  π(a=3|s=81) = 0.29  --> best action: 1\n",
      "  State 82: π(a=0|s=82) = 0.42  π(a=1|s=82) = 0.28  π(a=2|s=82) = 0.16  π(a=3|s=82) = 0.14  --> best action: 0\n",
      "  State 83: π(a=0|s=83) = 0.07  π(a=1|s=83) = 0.36  π(a=2|s=83) = 0.28  π(a=3|s=83) = 0.28  --> best action: 1\n",
      "  State 84: π(a=0|s=84) = 0.00  π(a=1|s=84) = 0.00  π(a=2|s=84) = 0.00  π(a=3|s=84) = 1.00  --> best action: 3\n",
      "  State 85: π(a=0|s=85) = 0.00  π(a=1|s=85) = 1.00  π(a=2|s=85) = 0.00  π(a=3|s=85) = 0.00  --> best action: 1\n",
      "  State 86: π(a=0|s=86) = 0.00  π(a=1|s=86) = 1.00  π(a=2|s=86) = 0.00  π(a=3|s=86) = 0.00  --> best action: 1\n",
      "  State 87: π(a=0|s=87) = 0.00  π(a=1|s=87) = 0.33  π(a=2|s=87) = 0.00  π(a=3|s=87) = 0.67  --> best action: 3\n",
      "  State 88: π(a=0|s=88) = 0.00  π(a=1|s=88) = 0.64  π(a=2|s=88) = 0.00  π(a=3|s=88) = 0.36  --> best action: 1\n",
      "  State 89: π(a=0|s=89) = 0.00  π(a=1|s=89) = 1.00  π(a=2|s=89) = 0.00  π(a=3|s=89) = 0.00  --> best action: 1\n",
      "  State 90: π(a=0|s=90) = 0.21  π(a=1|s=90) = 0.26  π(a=2|s=90) = 0.26  π(a=3|s=90) = 0.28  --> best action: 3\n",
      "  State 91: π(a=0|s=91) = 0.21  π(a=1|s=91) = 0.26  π(a=2|s=91) = 0.24  π(a=3|s=91) = 0.28  --> best action: 3\n",
      "  State 92: π(a=0|s=92) = 0.26  π(a=1|s=92) = 0.26  π(a=2|s=92) = 0.24  π(a=3|s=92) = 0.24  --> best action: 0\n",
      "  State 93: π(a=0|s=93) = 0.19  π(a=1|s=93) = 0.26  π(a=2|s=93) = 0.28  π(a=3|s=93) = 0.26  --> best action: 2\n",
      "  State 94: π(a=0|s=94) = 0.00  π(a=1|s=94) = 0.00  π(a=2|s=94) = 0.00  π(a=3|s=94) = 1.00  --> best action: 3\n",
      "  State 95: π(a=0|s=95) = 0.00  π(a=1|s=95) = 0.00  π(a=2|s=95) = 0.00  π(a=3|s=95) = 1.00  --> best action: 3\n",
      "  State 96: π(a=0|s=96) = 0.00  π(a=1|s=96) = 0.00  π(a=2|s=96) = 0.00  π(a=3|s=96) = 1.00  --> best action: 3\n",
      "  State 97: π(a=0|s=97) = 0.00  π(a=1|s=97) = 0.00  π(a=2|s=97) = 0.00  π(a=3|s=97) = 1.00  --> best action: 3\n",
      "  State 98: π(a=0|s=98) = 0.00  π(a=1|s=98) = 0.00  π(a=2|s=98) = 0.00  π(a=3|s=98) = 1.00  --> best action: 3\n",
      "  State 99: π(a=0|s=99) = 0.25  π(a=1|s=99) = 0.25  π(a=2|s=99) = 0.25  π(a=3|s=99) = 0.25  --> best action: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "solver.run(T = 10000, alpha=alpha_star, eta=eta_star, rho=rho_star, tqdm_print=True)\n",
    "evaluator.print_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db373d8",
   "metadata": {},
   "source": [
    "Let's do an easy check that the policy achieves the goal in a optimal way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5298402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "  OPTIMAL PATH VISUALIZATION - Learned Policy (π_FOGAS)\n",
      "======================================================================\n",
      "\n",
      "Initial State: 0\n",
      "Goal State: 99\n",
      "Discount Factor (γ): 0.9\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "  Step   0 │ State: 0        │ Action: 3       │ Reward:  -0.100 │ → 1\n",
      "  Step   1 │ State: 1        │ Action: 3       │ Reward:  -0.100 │ → 2\n",
      "  Step   2 │ State: 2        │ Action: 1       │ Reward:  -0.100 │ → 12\n",
      "  Step   3 │ State: 12       │ Action: 3       │ Reward:  -0.100 │ → 13\n",
      "  Step   4 │ State: 13       │ Action: 1       │ Reward:  -0.100 │ → 23\n",
      "  Step   5 │ State: 23       │ Action: 3       │ Reward:  -0.100 │ → 24\n",
      "  Step   6 │ State: 24       │ Action: 3       │ Reward:  -0.100 │ → 25\n",
      "  Step   7 │ State: 25       │ Action: 0       │ Reward:  -0.100 │ → 15\n",
      "  Step   8 │ State: 15       │ Action: 0       │ Reward:  -0.100 │ → 5\n",
      "  Step   9 │ State: 5        │ Action: 3       │ Reward:  -0.100 │ → 6\n",
      "  Step  10 │ State: 6        │ Action: 3       │ Reward:  -0.100 │ → 7\n",
      "  Step  11 │ State: 7        │ Action: 3       │ Reward:  -0.100 │ → 8\n",
      "  Step  12 │ State: 8        │ Action: 3       │ Reward:  -0.100 │ → 9\n",
      "  Step  13 │ State: 9        │ Action: 1       │ Reward:  -0.100 │ → 19\n",
      "  Step  14 │ State: 19       │ Action: 1       │ Reward:  -0.100 │ → 29\n",
      "  Step  15 │ State: 29       │ Action: 1       │ Reward:  -0.100 │ → 39\n",
      "  Step  16 │ State: 39       │ Action: 2       │ Reward:  -0.100 │ → 38\n",
      "  Step  17 │ State: 38       │ Action: 1       │ Reward:  -0.100 │ → 48\n",
      "  Step  18 │ State: 48       │ Action: 1       │ Reward:  -0.100 │ → 58\n",
      "  Step  19 │ State: 58       │ Action: 1       │ Reward:  -0.100 │ → 68\n",
      "  Step  20 │ State: 68       │ Action: 1       │ Reward:  -0.100 │ → 78\n",
      "  Step  21 │ State: 78       │ Action: 1       │ Reward:  -0.100 │ → 88\n",
      "  Step  22 │ State: 88       │ Action: 1       │ Reward:  -0.100 │ → 98\n",
      "  Step  23 │ State: 98       │ Action: 3       │ Reward:  -0.100 │ → 99 🎯 GOAL REACHED!\n",
      "\n",
      "  ──────────────────────────────────────────────────────────────────\n",
      "  Trajectory Length: 24 steps\n",
      "  Discounted Return: -0.920234\n",
      "  Final State: 99\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "  Expected Return (from V): -0.486260\n",
      "  Optimal Return (π*): -0.122569\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator.print_optimal_path(goal_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e550468",
   "metadata": {},
   "source": [
    "## Empirical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ec1f97",
   "metadata": {},
   "source": [
    "### Dataset Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5453ce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the collector with these options\n",
    "collector = EnvDataCollector(\n",
    "    mdp=mdp,\n",
    "    env_name=\"10grid_wall\",\n",
    "    reset_probs={'custom': 1.0},\n",
    "    max_steps=50,\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40a7666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  MIXED DATASET COLLECTION SUMMARY (TORCH)\n",
      "============================================================\n",
      "Total transitions: 8000\n",
      "Total episodes: 242\n",
      "Mode: Episode-based\n",
      "\n",
      "Policy Distribution:\n",
      "  Policy 0:  6047 steps (75.6%) | Target: 80.0% | Episodes: 194\n",
      "  Policy 1:  1953 steps (24.4%) | Target: 20.0% | Episodes: 49\n",
      "============================================================\n",
      "\n",
      "✅ Mixed dataset saved to: /shared/home/mauro.diaz/work/FOGAS/datasets/10grid_wall.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>step</th>\n",
       "      <th>state</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>next_state</th>\n",
       "      <th>policy_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>242</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>242</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>242</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>242</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>242</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      episode  step  state  action  reward  next_state  policy_id\n",
       "0           0     0      0       0    -0.1           0          0\n",
       "1           0     1      0       3    -0.1           1          0\n",
       "2           0     2      1       3    -0.1           2          0\n",
       "3           0     3      2       1    -0.1          12          0\n",
       "4           0     4     12       3    -0.1          13          0\n",
       "...       ...   ...    ...     ...     ...         ...        ...\n",
       "7995      242     9      3       3    -0.1           3          1\n",
       "7996      242    10      3       2    -0.1           2          1\n",
       "7997      242    11      2       0    -0.1           2          1\n",
       "7998      242    12      2       0    -0.1           2          1\n",
       "7999      242    13      2       3    -0.1           3          1\n",
       "\n",
       "[8000 rows x 7 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an epsilon-greedy pi_star (30% optimal, 70% random)\n",
    "epsilon_policy = (mdp.pi_star, 0.3)\n",
    "\n",
    "# Use it in your mixed collection as before\n",
    "collector.collect_mixed_dataset(\n",
    "    policies=[epsilon_policy, \"random\"],  \n",
    "    proportions=[0.8, 0.2],\n",
    "    n_steps=8000,\n",
    "    episode_based=True,                   \n",
    "    save_path=str(DATASET_PATH),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1a95eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Dataset Analysis Summary\n",
      "==================================================\n",
      "Source: /home/mauro/Desktop/EMAI/Ljubljana/Thesis/Code/datasets/10grid_wall.csv\n",
      "Total transitions: 8,000\n",
      "Unique (state, action) pairs: 195\n",
      "Unique states visited: 53\n",
      "Unique actions taken: 4\n",
      "--------------------------------------------------\n",
      "Pair Frequency Statistics:\n",
      "  Min count:    1\n",
      "  Max count:    300\n",
      "  Mean count:   41.03\n",
      "  Std count:    62.54\n",
      "  Median count: 17.0\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "analyzer = DatasetAnalyzer(DATASET_PATH)\n",
    "\n",
    "analyzer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d60748",
   "metadata": {},
   "source": [
    "### Solving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d3a05ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver_e = FOGASSolverVectorized(\n",
    "    mdp=mdp,\n",
    "    csv_path=str(DATASET_PATH),\n",
    "    device=device,\n",
    "    seed=seed)\n",
    "evaluator_e = FOGASEvaluator(solver_e)\n",
    "optimizer_e = FOGASHyperOptimizer(solver=solver_e,metric_callable=evaluator_e.get_metric(\"reward\"), seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0d9cd7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOGAS: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 12000/12000 [00:17<00:00, 681.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  State 0: π(a=0|s=0) = 0.13  π(a=1|s=0) = 0.17  π(a=2|s=0) = 0.12  π(a=3|s=0) = 0.58  --> best action: 3\n",
      "  State 1: π(a=0|s=1) = 0.13  π(a=1|s=1) = 0.14  π(a=2|s=1) = 0.05  π(a=3|s=1) = 0.68  --> best action: 3\n",
      "  State 2: π(a=0|s=2) = 0.14  π(a=1|s=2) = 0.63  π(a=2|s=2) = 0.06  π(a=3|s=2) = 0.18  --> best action: 1\n",
      "  State 3: π(a=0|s=3) = 0.25  π(a=1|s=3) = 0.46  π(a=2|s=3) = 0.09  π(a=3|s=3) = 0.20  --> best action: 1\n",
      "  State 4: π(a=0|s=4) = 0.25  π(a=1|s=4) = 0.25  π(a=2|s=4) = 0.25  π(a=3|s=4) = 0.25  --> best action: 0\n",
      "  State 5: π(a=0|s=5) = 0.15  π(a=1|s=5) = 0.07  π(a=2|s=5) = 0.13  π(a=3|s=5) = 0.65  --> best action: 3\n",
      "  State 6: π(a=0|s=6) = 0.11  π(a=1|s=6) = 0.07  π(a=2|s=6) = 0.07  π(a=3|s=6) = 0.75  --> best action: 3\n",
      "  State 7: π(a=0|s=7) = 0.13  π(a=1|s=7) = 0.09  π(a=2|s=7) = 0.06  π(a=3|s=7) = 0.73  --> best action: 3\n",
      "  State 8: π(a=0|s=8) = 0.11  π(a=1|s=8) = 0.03  π(a=2|s=8) = 0.05  π(a=3|s=8) = 0.81  --> best action: 3\n",
      "  State 9: π(a=0|s=9) = 0.10  π(a=1|s=9) = 0.70  π(a=2|s=9) = 0.06  π(a=3|s=9) = 0.14  --> best action: 1\n",
      "  State 10: π(a=0|s=10) = 0.25  π(a=1|s=10) = 0.32  π(a=2|s=10) = 0.22  π(a=3|s=10) = 0.22  --> best action: 1\n",
      "  State 11: π(a=0|s=11) = 0.25  π(a=1|s=11) = 0.25  π(a=2|s=11) = 0.25  π(a=3|s=11) = 0.25  --> best action: 0\n",
      "  State 12: π(a=0|s=12) = 0.08  π(a=1|s=12) = 0.15  π(a=2|s=12) = 0.15  π(a=3|s=12) = 0.62  --> best action: 3\n",
      "  State 13: π(a=0|s=13) = 0.06  π(a=1|s=13) = 0.75  π(a=2|s=13) = 0.08  π(a=3|s=13) = 0.11  --> best action: 1\n",
      "  State 14: π(a=0|s=14) = 0.25  π(a=1|s=14) = 0.25  π(a=2|s=14) = 0.25  π(a=3|s=14) = 0.25  --> best action: 0\n",
      "  State 15: π(a=0|s=15) = 0.12  π(a=1|s=15) = 0.07  π(a=2|s=15) = 0.10  π(a=3|s=15) = 0.71  --> best action: 3\n",
      "  State 16: π(a=0|s=16) = 0.75  π(a=1|s=16) = 0.06  π(a=2|s=16) = 0.07  π(a=3|s=16) = 0.12  --> best action: 0\n",
      "  State 17: π(a=0|s=17) = 0.25  π(a=1|s=17) = 0.25  π(a=2|s=17) = 0.25  π(a=3|s=17) = 0.25  --> best action: 0\n",
      "  State 18: π(a=0|s=18) = 0.25  π(a=1|s=18) = 0.25  π(a=2|s=18) = 0.25  π(a=3|s=18) = 0.25  --> best action: 0\n",
      "  State 19: π(a=0|s=19) = 0.06  π(a=1|s=19) = 0.81  π(a=2|s=19) = 0.00  π(a=3|s=19) = 0.12  --> best action: 1\n",
      "  State 20: π(a=0|s=20) = 0.11  π(a=1|s=20) = 0.37  π(a=2|s=20) = 0.27  π(a=3|s=20) = 0.25  --> best action: 1\n",
      "  State 21: π(a=0|s=21) = 0.25  π(a=1|s=21) = 0.25  π(a=2|s=21) = 0.25  π(a=3|s=21) = 0.25  --> best action: 0\n",
      "  State 22: π(a=0|s=22) = 0.25  π(a=1|s=22) = 0.25  π(a=2|s=22) = 0.25  π(a=3|s=22) = 0.25  --> best action: 0\n",
      "  State 23: π(a=0|s=23) = 0.05  π(a=1|s=23) = 0.08  π(a=2|s=23) = 0.08  π(a=3|s=23) = 0.79  --> best action: 3\n",
      "  State 24: π(a=0|s=24) = 0.12  π(a=1|s=24) = 0.09  π(a=2|s=24) = 0.04  π(a=3|s=24) = 0.76  --> best action: 3\n",
      "  State 25: π(a=0|s=25) = 0.68  π(a=1|s=25) = 0.12  π(a=2|s=25) = 0.04  π(a=3|s=25) = 0.16  --> best action: 0\n",
      "  State 26: π(a=0|s=26) = 0.71  π(a=1|s=26) = 0.09  π(a=2|s=26) = 0.09  π(a=3|s=26) = 0.11  --> best action: 0\n",
      "  State 27: π(a=0|s=27) = 0.25  π(a=1|s=27) = 0.25  π(a=2|s=27) = 0.25  π(a=3|s=27) = 0.25  --> best action: 0\n",
      "  State 28: π(a=0|s=28) = 0.00  π(a=1|s=28) = 0.77  π(a=2|s=28) = 0.11  π(a=3|s=28) = 0.12  --> best action: 1\n",
      "  State 29: π(a=0|s=29) = 0.06  π(a=1|s=29) = 0.64  π(a=2|s=29) = 0.17  π(a=3|s=29) = 0.13  --> best action: 1\n",
      "  State 30: π(a=0|s=30) = 0.08  π(a=1|s=30) = 0.25  π(a=2|s=30) = 0.14  π(a=3|s=30) = 0.53  --> best action: 3\n",
      "  State 31: π(a=0|s=31) = 0.20  π(a=1|s=31) = 0.67  π(a=2|s=31) = 0.06  π(a=3|s=31) = 0.08  --> best action: 1\n",
      "  State 32: π(a=0|s=32) = 0.25  π(a=1|s=32) = 0.25  π(a=2|s=32) = 0.25  π(a=3|s=32) = 0.25  --> best action: 0\n",
      "  State 33: π(a=0|s=33) = 0.60  π(a=1|s=33) = 0.18  π(a=2|s=33) = 0.01  π(a=3|s=33) = 0.20  --> best action: 0\n",
      "  State 34: π(a=0|s=34) = 0.25  π(a=1|s=34) = 0.25  π(a=2|s=34) = 0.25  π(a=3|s=34) = 0.25  --> best action: 0\n",
      "  State 35: π(a=0|s=35) = 0.43  π(a=1|s=35) = 0.19  π(a=2|s=35) = 0.22  π(a=3|s=35) = 0.17  --> best action: 0\n",
      "  State 36: π(a=0|s=36) = 0.56  π(a=1|s=36) = 0.24  π(a=2|s=36) = 0.16  π(a=3|s=36) = 0.04  --> best action: 0\n",
      "  State 37: π(a=0|s=37) = 0.25  π(a=1|s=37) = 0.25  π(a=2|s=37) = 0.25  π(a=3|s=37) = 0.25  --> best action: 0\n",
      "  State 38: π(a=0|s=38) = 0.04  π(a=1|s=38) = 0.75  π(a=2|s=38) = 0.14  π(a=3|s=38) = 0.07  --> best action: 1\n",
      "  State 39: π(a=0|s=39) = 0.07  π(a=1|s=39) = 0.14  π(a=2|s=39) = 0.65  π(a=3|s=39) = 0.14  --> best action: 2\n",
      "  State 40: π(a=0|s=40) = 0.25  π(a=1|s=40) = 0.25  π(a=2|s=40) = 0.25  π(a=3|s=40) = 0.25  --> best action: 0\n",
      "  State 41: π(a=0|s=41) = 0.02  π(a=1|s=41) = 0.62  π(a=2|s=41) = 0.20  π(a=3|s=41) = 0.16  --> best action: 1\n",
      "  State 42: π(a=0|s=42) = 0.25  π(a=1|s=42) = 0.25  π(a=2|s=42) = 0.25  π(a=3|s=42) = 0.25  --> best action: 0\n",
      "  State 43: π(a=0|s=43) = 0.25  π(a=1|s=43) = 0.25  π(a=2|s=43) = 0.25  π(a=3|s=43) = 0.25  --> best action: 0\n",
      "  State 44: π(a=0|s=44) = 0.25  π(a=1|s=44) = 0.25  π(a=2|s=44) = 0.25  π(a=3|s=44) = 0.25  --> best action: 0\n",
      "  State 45: π(a=0|s=45) = 0.25  π(a=1|s=45) = 0.25  π(a=2|s=45) = 0.25  π(a=3|s=45) = 0.25  --> best action: 0\n",
      "  State 46: π(a=0|s=46) = 0.25  π(a=1|s=46) = 0.25  π(a=2|s=46) = 0.25  π(a=3|s=46) = 0.25  --> best action: 0\n",
      "  State 47: π(a=0|s=47) = 0.25  π(a=1|s=47) = 0.25  π(a=2|s=47) = 0.25  π(a=3|s=47) = 0.25  --> best action: 0\n",
      "  State 48: π(a=0|s=48) = 0.05  π(a=1|s=48) = 0.69  π(a=2|s=48) = 0.13  π(a=3|s=48) = 0.12  --> best action: 1\n",
      "  State 49: π(a=0|s=49) = 0.25  π(a=1|s=49) = 0.25  π(a=2|s=49) = 0.25  π(a=3|s=49) = 0.25  --> best action: 0\n",
      "  State 50: π(a=0|s=50) = 0.35  π(a=1|s=50) = 0.32  π(a=2|s=50) = 0.32  π(a=3|s=50) = 0.00  --> best action: 0\n",
      "  State 51: π(a=0|s=51) = 0.01  π(a=1|s=51) = 0.29  π(a=2|s=51) = 0.21  π(a=3|s=51) = 0.50  --> best action: 3\n",
      "  State 52: π(a=0|s=52) = 0.19  π(a=1|s=52) = 0.16  π(a=2|s=52) = 0.00  π(a=3|s=52) = 0.65  --> best action: 3\n",
      "  State 53: π(a=0|s=53) = 0.16  π(a=1|s=53) = 0.64  π(a=2|s=53) = 0.00  π(a=3|s=53) = 0.19  --> best action: 1\n",
      "  State 54: π(a=0|s=54) = 0.25  π(a=1|s=54) = 0.25  π(a=2|s=54) = 0.25  π(a=3|s=54) = 0.25  --> best action: 0\n",
      "  State 55: π(a=0|s=55) = 0.25  π(a=1|s=55) = 0.25  π(a=2|s=55) = 0.25  π(a=3|s=55) = 0.25  --> best action: 0\n",
      "  State 56: π(a=0|s=56) = 0.25  π(a=1|s=56) = 0.25  π(a=2|s=56) = 0.25  π(a=3|s=56) = 0.25  --> best action: 0\n",
      "  State 57: π(a=0|s=57) = 0.25  π(a=1|s=57) = 0.25  π(a=2|s=57) = 0.25  π(a=3|s=57) = 0.25  --> best action: 0\n",
      "  State 58: π(a=0|s=58) = 0.05  π(a=1|s=58) = 0.64  π(a=2|s=58) = 0.12  π(a=3|s=58) = 0.19  --> best action: 1\n",
      "  State 59: π(a=0|s=59) = 0.15  π(a=1|s=59) = 0.53  π(a=2|s=59) = 0.13  π(a=3|s=59) = 0.18  --> best action: 1\n",
      "  State 60: π(a=0|s=60) = 0.25  π(a=1|s=60) = 0.25  π(a=2|s=60) = 0.25  π(a=3|s=60) = 0.25  --> best action: 0\n",
      "  State 61: π(a=0|s=61) = 0.25  π(a=1|s=61) = 0.25  π(a=2|s=61) = 0.25  π(a=3|s=61) = 0.25  --> best action: 0\n",
      "  State 62: π(a=0|s=62) = 0.25  π(a=1|s=62) = 0.25  π(a=2|s=62) = 0.25  π(a=3|s=62) = 0.25  --> best action: 0\n",
      "  State 63: π(a=0|s=63) = 0.02  π(a=1|s=63) = 0.52  π(a=2|s=63) = 0.20  π(a=3|s=63) = 0.25  --> best action: 1\n",
      "  State 64: π(a=0|s=64) = 0.25  π(a=1|s=64) = 0.25  π(a=2|s=64) = 0.25  π(a=3|s=64) = 0.25  --> best action: 0\n",
      "  State 65: π(a=0|s=65) = 0.25  π(a=1|s=65) = 0.25  π(a=2|s=65) = 0.25  π(a=3|s=65) = 0.25  --> best action: 0\n",
      "  State 66: π(a=0|s=66) = 0.25  π(a=1|s=66) = 0.25  π(a=2|s=66) = 0.25  π(a=3|s=66) = 0.25  --> best action: 0\n",
      "  State 67: π(a=0|s=67) = 0.26  π(a=1|s=67) = 0.39  π(a=2|s=67) = 0.22  π(a=3|s=67) = 0.13  --> best action: 1\n",
      "  State 68: π(a=0|s=68) = 0.05  π(a=1|s=68) = 0.53  π(a=2|s=68) = 0.26  π(a=3|s=68) = 0.16  --> best action: 1\n",
      "  State 69: π(a=0|s=69) = 0.08  π(a=1|s=69) = 0.59  π(a=2|s=69) = 0.15  π(a=3|s=69) = 0.18  --> best action: 1\n",
      "  State 70: π(a=0|s=70) = 0.25  π(a=1|s=70) = 0.25  π(a=2|s=70) = 0.25  π(a=3|s=70) = 0.25  --> best action: 0\n",
      "  State 71: π(a=0|s=71) = 0.25  π(a=1|s=71) = 0.25  π(a=2|s=71) = 0.25  π(a=3|s=71) = 0.25  --> best action: 0\n",
      "  State 72: π(a=0|s=72) = 0.25  π(a=1|s=72) = 0.25  π(a=2|s=72) = 0.25  π(a=3|s=72) = 0.25  --> best action: 0\n",
      "  State 73: π(a=0|s=73) = 0.29  π(a=1|s=73) = 0.23  π(a=2|s=73) = 0.29  π(a=3|s=73) = 0.19  --> best action: 0\n",
      "  State 74: π(a=0|s=74) = 0.29  π(a=1|s=74) = 0.29  π(a=2|s=74) = 0.12  π(a=3|s=74) = 0.29  --> best action: 0\n",
      "  State 75: π(a=0|s=75) = 0.25  π(a=1|s=75) = 0.25  π(a=2|s=75) = 0.25  π(a=3|s=75) = 0.25  --> best action: 0\n",
      "  State 76: π(a=0|s=76) = 0.25  π(a=1|s=76) = 0.25  π(a=2|s=76) = 0.25  π(a=3|s=76) = 0.25  --> best action: 0\n",
      "  State 77: π(a=0|s=77) = 0.15  π(a=1|s=77) = 0.46  π(a=2|s=77) = 0.29  π(a=3|s=77) = 0.11  --> best action: 1\n",
      "  State 78: π(a=0|s=78) = 0.06  π(a=1|s=78) = 0.48  π(a=2|s=78) = 0.24  π(a=3|s=78) = 0.22  --> best action: 1\n",
      "  State 79: π(a=0|s=79) = 0.06  π(a=1|s=79) = 0.62  π(a=2|s=79) = 0.13  π(a=3|s=79) = 0.20  --> best action: 1\n",
      "  State 80: π(a=0|s=80) = 0.25  π(a=1|s=80) = 0.25  π(a=2|s=80) = 0.25  π(a=3|s=80) = 0.25  --> best action: 0\n",
      "  State 81: π(a=0|s=81) = 0.25  π(a=1|s=81) = 0.25  π(a=2|s=81) = 0.25  π(a=3|s=81) = 0.25  --> best action: 0\n",
      "  State 82: π(a=0|s=82) = 0.25  π(a=1|s=82) = 0.25  π(a=2|s=82) = 0.25  π(a=3|s=82) = 0.25  --> best action: 0\n",
      "  State 83: π(a=0|s=83) = 0.60  π(a=1|s=83) = 0.13  π(a=2|s=83) = 0.14  π(a=3|s=83) = 0.14  --> best action: 0\n",
      "  State 84: π(a=0|s=84) = 0.25  π(a=1|s=84) = 0.25  π(a=2|s=84) = 0.25  π(a=3|s=84) = 0.25  --> best action: 0\n",
      "  State 85: π(a=0|s=85) = 0.25  π(a=1|s=85) = 0.25  π(a=2|s=85) = 0.25  π(a=3|s=85) = 0.25  --> best action: 0\n",
      "  State 86: π(a=0|s=86) = 0.25  π(a=1|s=86) = 0.25  π(a=2|s=86) = 0.25  π(a=3|s=86) = 0.25  --> best action: 0\n",
      "  State 87: π(a=0|s=87) = 0.25  π(a=1|s=87) = 0.25  π(a=2|s=87) = 0.25  π(a=3|s=87) = 0.25  --> best action: 0\n",
      "  State 88: π(a=0|s=88) = 0.06  π(a=1|s=88) = 0.46  π(a=2|s=88) = 0.16  π(a=3|s=88) = 0.32  --> best action: 1\n",
      "  State 89: π(a=0|s=89) = 0.05  π(a=1|s=89) = 0.72  π(a=2|s=89) = 0.07  π(a=3|s=89) = 0.16  --> best action: 1\n",
      "  State 90: π(a=0|s=90) = 0.25  π(a=1|s=90) = 0.25  π(a=2|s=90) = 0.25  π(a=3|s=90) = 0.25  --> best action: 0\n",
      "  State 91: π(a=0|s=91) = 0.25  π(a=1|s=91) = 0.25  π(a=2|s=91) = 0.25  π(a=3|s=91) = 0.25  --> best action: 0\n",
      "  State 92: π(a=0|s=92) = 0.25  π(a=1|s=92) = 0.25  π(a=2|s=92) = 0.25  π(a=3|s=92) = 0.25  --> best action: 0\n",
      "  State 93: π(a=0|s=93) = 0.15  π(a=1|s=93) = 0.14  π(a=2|s=93) = 0.58  π(a=3|s=93) = 0.14  --> best action: 2\n",
      "  State 94: π(a=0|s=94) = 0.25  π(a=1|s=94) = 0.25  π(a=2|s=94) = 0.25  π(a=3|s=94) = 0.25  --> best action: 0\n",
      "  State 95: π(a=0|s=95) = 0.25  π(a=1|s=95) = 0.25  π(a=2|s=95) = 0.25  π(a=3|s=95) = 0.25  --> best action: 0\n",
      "  State 96: π(a=0|s=96) = 0.28  π(a=1|s=96) = 0.28  π(a=2|s=96) = 0.28  π(a=3|s=96) = 0.16  --> best action: 0\n",
      "  State 97: π(a=0|s=97) = 0.14  π(a=1|s=97) = 0.66  π(a=2|s=97) = 0.13  π(a=3|s=97) = 0.08  --> best action: 1\n",
      "  State 98: π(a=0|s=98) = 0.05  π(a=1|s=98) = 0.14  π(a=2|s=98) = 0.25  π(a=3|s=98) = 0.55  --> best action: 3\n",
      "  State 99: π(a=0|s=99) = 0.25  π(a=1|s=99) = 0.25  π(a=2|s=99) = 0.25  π(a=3|s=99) = 0.25  --> best action: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run solver with best alpha and fixed rho, eta\n",
    "solver_e.run(alpha=0.001 / 2, eta=0.0002, rho=0.05, tqdm_print=True, T=12000)\n",
    "evaluator_e.print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1101c0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "  OPTIMAL PATH VISUALIZATION - Learned Policy (π_FOGAS)\n",
      "======================================================================\n",
      "\n",
      "Initial State: 0\n",
      "Goal State: 99\n",
      "Discount Factor (γ): 0.9\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "  Step   0 │ State: 0        │ Action: 3       │ Reward:  -0.100 │ → 1\n",
      "  Step   1 │ State: 1        │ Action: 3       │ Reward:  -0.100 │ → 2\n",
      "  Step   2 │ State: 2        │ Action: 1       │ Reward:  -0.100 │ → 12\n",
      "  Step   3 │ State: 12       │ Action: 3       │ Reward:  -0.100 │ → 13\n",
      "  Step   4 │ State: 13       │ Action: 1       │ Reward:  -0.100 │ → 23\n",
      "  Step   5 │ State: 23       │ Action: 3       │ Reward:  -0.100 │ → 24\n",
      "  Step   6 │ State: 24       │ Action: 3       │ Reward:  -0.100 │ → 25\n",
      "  Step   7 │ State: 25       │ Action: 0       │ Reward:  -0.100 │ → 15\n",
      "  Step   8 │ State: 15       │ Action: 3       │ Reward:  -0.100 │ → 16\n",
      "  Step   9 │ State: 16       │ Action: 0       │ Reward:  -0.100 │ → 6\n",
      "  Step  10 │ State: 6        │ Action: 3       │ Reward:  -0.100 │ → 7\n",
      "  Step  11 │ State: 7        │ Action: 3       │ Reward:  -0.100 │ → 8\n",
      "  Step  12 │ State: 8        │ Action: 3       │ Reward:  -0.100 │ → 9\n",
      "  Step  13 │ State: 9        │ Action: 1       │ Reward:  -0.100 │ → 19\n",
      "  Step  14 │ State: 19       │ Action: 1       │ Reward:  -0.100 │ → 29\n",
      "  Step  15 │ State: 29       │ Action: 1       │ Reward:  -0.100 │ → 39\n",
      "  Step  16 │ State: 39       │ Action: 2       │ Reward:  -0.100 │ → 38\n",
      "  Step  17 │ State: 38       │ Action: 1       │ Reward:  -0.100 │ → 48\n",
      "  Step  18 │ State: 48       │ Action: 1       │ Reward:  -0.100 │ → 58\n",
      "  Step  19 │ State: 58       │ Action: 1       │ Reward:  -0.100 │ → 68\n",
      "  Step  20 │ State: 68       │ Action: 1       │ Reward:  -0.100 │ → 78\n",
      "  Step  21 │ State: 78       │ Action: 1       │ Reward:  -0.100 │ → 88\n",
      "  Step  22 │ State: 88       │ Action: 1       │ Reward:  -0.100 │ → 98\n",
      "  Step  23 │ State: 98       │ Action: 3       │ Reward:  -0.100 │ → 99 🎯 GOAL REACHED!\n",
      "\n",
      "  ──────────────────────────────────────────────────────────────────\n",
      "  Trajectory Length: 24 steps\n",
      "  Discounted Return: -0.920234\n",
      "  Final State: 99\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "  Expected Return (from V): -2.576409\n",
      "  Optimal Return (π*): -0.122569\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator_e.print_optimal_path(goal_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7348e0",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a77f062",
   "metadata": {},
   "source": [
    "**What “alignment-based coverage” looks like in this tabular case**\n",
    "\n",
    "FOGAS’ key coverage term is the **feature coverage ratio**\n",
    "$$\n",
    "|\\lambda_{\\pi^*}|_{\\Lambda_n^{-1}}^2\n",
    "$$\n",
    "(and this measures how well the dataset covers the comparator policy in feature space). \n",
    "\n",
    "With one-hot features, the empirical covariance\n",
    "$$\n",
    "\\Lambda_n = \\beta I + \\frac{1}{n}\\sum_{i=1}^n \\phi_i\\phi_i^\\top\n",
    "$$\n",
    "is **diagonal**, and each diagonal entry corresponds to a particular $(x,a)$:\n",
    "$$\n",
    "(\\Lambda_n)_{(x,a),(x,a)} = \\beta + \\frac{N(x,a)}{n}.\n",
    "$$\n",
    "(Here $N(x,a)$ is how often $(x,a)$ appears in the dataset.) \n",
    "\n",
    "Because it’s diagonal, the coverage ratio becomes:\n",
    "$$\n",
    "|\\lambda_{\\pi^*}|_{\\Lambda_n^{-1}}^2 =\n",
    "\\sum_{x,a}\\frac{\\mu_{\\pi^*}(x,a)^2}{\\beta + N(x,a)/n}.\n",
    "$$\n",
    "\n",
    "So **alignment-based coverage** (in tabular) literally means:\n",
    "\n",
    "- The dataset counts $N(x,a)$ are large exactly on the $(x,a)$ where $\\mu_{\\pi^*}(x,a)$ is large.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55e89da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Coverage Ratio Details\n",
      "------------------------------\n",
      "  Dataset size (n):         8000\n",
      "  MDP dims (N, A, d):        (100, 4, 400)\n",
      "  gamma:                    0.9\n",
      "  beta (ridge):             3.3761e-07\n",
      "  policy source:            provided\n",
      "\n",
      "  Occupancy μ_pi summary:\n",
      "    sum:                    1\n",
      "    min / max:              0 / 0.1\n",
      "\n",
      "  Feature occupancy λ_pi summary:\n",
      "    ||λ||_2:                0.242197\n",
      "    ||λ||_1:                1\n",
      "    max |λ_i|:              0.1\n",
      "\n",
      "  Empirical covariance Λ_n:\n",
      "    diag min / mean / max:  3.3761e-07 / 0.00250034 / 0.0375003\n",
      "    eig min / max:          3.3761e-07 / 0.0375003\n",
      "    condition number:       111076\n",
      "\n",
      "  Coverage ratio:           18849.2\n"
     ]
    }
   ],
   "source": [
    "ratio = analyzer.feature_coverage_ratio(\n",
    "    mdp=mdp,\n",
    "    beta=solver_e.params.beta,\n",
    "    use_optimal_policy=True,\n",
    "    verbose=True,\n",
    "    return_details=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1055a53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Grid Search with beta=3.38e-07 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search:   1%|          | 1/100 [00:03<05:02,  3.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search:  17%|█▋        | 17/100 [03:30<17:07, 12.38s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(save_path):\n\u001b[32m     70\u001b[39m     os.remove(save_path)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[43mcollector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect_mixed_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpolicies\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mepsilon_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrandom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproportions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprops\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepisode_based\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     79\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# 2. Analyze Coverage Ratio\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# Using the Analyzer to compute coverage\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/EMAI/Ljubljana/Thesis/Code/fogas_torch/dataset_collection/env_data_collector.py:356\u001b[39m, in \u001b[36mEnvDataCollector.collect_mixed_dataset\u001b[39m\u001b[34m(self, policies, proportions, n_steps, save_path, verbose, episode_based)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;66;03m# Execute action\u001b[39;00m\n\u001b[32m    355\u001b[39m action = policy.sample(obs)\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m next_obs, reward, terminated, truncated, _ = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[38;5;66;03m# Record transition\u001b[39;00m\n\u001b[32m    359\u001b[39m data[\u001b[33m\"\u001b[39m\u001b[33mepisode\u001b[39m\u001b[33m\"\u001b[39m].append(episode)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/EMAI/Ljubljana/Thesis/Code/fogas_torch/dataset_collection/linear_mdp_env.py:169\u001b[39m, in \u001b[36mLinearMDPEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    167\u001b[39m probs = \u001b[38;5;28mself\u001b[39m.P[row_idx]\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# Sample next state using torch.multinomial\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m next_state_idx = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m.item()\n\u001b[32m    170\u001b[39m next_state = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m.states[next_state_idx].item())\n\u001b[32m    171\u001b[39m reward = \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m.r[row_idx].item())\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "# Use standard text-based tqdm to avoid ipywidgets/IProgress errors\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. Define Grid Search Parameters ---\n",
    "# Vary dataset sizes around your baseline of 8000\n",
    "dataset_sizes = [8000, 12000, 16000, 20000, 24000]\n",
    "\n",
    "# Vary epsilon for the epsilon-greedy policy (prob of random action)\n",
    "epsilon_values = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "\n",
    "# Vary proportions: [Epsilon-Greedy, Random]\n",
    "# Note: Epsilon-Greedy already has randomness, but this mixes it with pure Random policy\n",
    "proportion_configs = [\n",
    "    ([1.0, 0], \"100% Eps-Greedy / 0% Random\"),\n",
    "    ([0.9, 0.1], \"90% Eps-Greedy / 10% Random\"),\n",
    "    ([0.8, 0.2], \"80% Eps-Greedy / 20% Random\"),\n",
    "    ([0.7, 0.3], \"70% Eps-Greedy / 30% Random\"),\n",
    "]\n",
    "\n",
    "# Attempt to retrieve beta from solver (solver_e or solver)\n",
    "try:\n",
    "    # Try the name 'solver_e'\n",
    "    beta_val = solver_e.params.beta\n",
    "except NameError:\n",
    "    try:\n",
    "        # Try the name 'solver' (common in your previous cells)\n",
    "        beta_val = solver.params.beta\n",
    "    except:\n",
    "        print(\"Warning: Could not find 'solver_e' or 'solver' for beta. Using default 1e-4.\")\n",
    "        beta_val = 1e-4\n",
    "\n",
    "# Ensure beta is a float (converts from tensor if needed)\n",
    "if hasattr(beta_val, \"item\"):\n",
    "    beta_val = beta_val.item()\n",
    "beta_val = float(beta_val)\n",
    "\n",
    "print(f\"Running Grid Search with beta={beta_val:.2e} ...\")\n",
    "\n",
    "# Create temp directory\n",
    "temp_dir = \"temp_datasets_grid_search\"\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "# --- 2. Grid Search Loop ---\n",
    "# Calculate total iterations for progress bar\n",
    "total_iters = len(dataset_sizes) * len(epsilon_values) * len(proportion_configs)\n",
    "\n",
    "# Use text-based tqdm\n",
    "with tqdm(total=total_iters, desc=\"Grid Search\") as pbar:\n",
    "    for props, prop_label in proportion_configs:\n",
    "        for eps in epsilon_values:\n",
    "            # Policy is a tuple: (Optimal Policy Matrix, epsilon)\n",
    "            # Assuming mdp.pi_star exists\n",
    "            epsilon_policy = (mdp.pi_star, eps)\n",
    "            \n",
    "            for n_steps in dataset_sizes:\n",
    "                # Unique filename for this run\n",
    "                fname = f\"data_n{n_steps}_eps{eps}_p{int(props[0]*100)}.csv\"\n",
    "                save_path = os.path.join(temp_dir, fname)\n",
    "                \n",
    "                # 1. Collect Dataset\n",
    "                # Clean up previous file if it exists\n",
    "                if os.path.exists(save_path):\n",
    "                    os.remove(save_path)\n",
    "                    \n",
    "                collector.collect_mixed_dataset(\n",
    "                    policies=[epsilon_policy, \"random\"],\n",
    "                    proportions=props,\n",
    "                    n_steps=n_steps,\n",
    "                    episode_based=True,\n",
    "                    save_path=save_path,\n",
    "                    verbose=False\n",
    "                )\n",
    "                \n",
    "                # 2. Analyze Coverage Ratio\n",
    "                try:\n",
    "                    # Using the Analyzer to compute coverage\n",
    "                    analyzer_instance = DatasetAnalyzer(save_path)\n",
    "                    \n",
    "                    ratio = analyzer_instance.feature_coverage_ratio(\n",
    "                        mdp=mdp,\n",
    "                        beta=beta_val,\n",
    "                        use_optimal_policy=True,\n",
    "                        verbose=False, # Reduce noise\n",
    "                        return_details=False\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error analyzing {fname}: {e}\")\n",
    "                    ratio = np.nan\n",
    "                \n",
    "                # 3. Store Result\n",
    "                results.append({\n",
    "                    \"Dataset Size\": n_steps,\n",
    "                    \"Epsilon\": eps,\n",
    "                    \"Proportions\": prop_label,\n",
    "                    \"Coverage Ratio\": ratio\n",
    "                })\n",
    "                \n",
    "                # Cleanup: remove temp file to save space\n",
    "                if os.path.exists(save_path):\n",
    "                    os.remove(save_path)\n",
    "                    \n",
    "                pbar.update(1)\n",
    "\n",
    "# --- 3. Plotting Results ---\n",
    "if results:\n",
    "    df_res = pd.DataFrame(results)\n",
    "    \n",
    "    # Setup plot style\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    \n",
    "    # Create FacetGrid: columns by Proportions, Color by Epsilon\n",
    "    g = sns.FacetGrid(df_res, col=\"Proportions\", hue=\"Epsilon\", height=5, aspect=1.2, sharey=True)\n",
    "    g.map(sns.lineplot, \"Dataset Size\", \"Coverage Ratio\", marker=\"o\", linewidth=2.5)\n",
    "    \n",
    "    # Polish axes and legends\n",
    "    g.add_legend(title=\"Epsilon (ε)\")\n",
    "    g.set_axis_labels(\"Dataset Size\", \"Coverage Ratio\")\n",
    "    g.fig.subplots_adjust(top=0.85)\n",
    "    g.fig.suptitle(f\"Feature Coverage Ratio vs Dataset Size (beta={beta_val:.2e})\", fontsize=16)\n",
    "    \n",
    "    # Set x-ticks to match sampled sizes\n",
    "    for ax in g.axes.flat:\n",
    "        ax.set_xticks(dataset_sizes)\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    # Display top results\n",
    "    print(\"\\nBest Configurations (Lowest Coverage Ratio):\")\n",
    "    print(df_res.sort_values(by=\"Coverage Ratio\", ascending=True).head(5))\n",
    "else:\n",
    "    print(\"No results generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e4c4db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fogas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
